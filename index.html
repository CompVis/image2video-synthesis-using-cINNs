<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>
      Stochastic Image-to-Video Synthesis using cINNs
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          Stochastic Image-to-Video Synthesis using cINNs
        </h2>
        <p>
        <a href="https://mdork.github.io/" style="color: #fff">Michael Dorkenwald</a><sup style="color: #fff">1</sup>,
        <a href="hhttps://timomilbich.github.io/" style="color: #fff">Timo Milbich</a><sup style="color: #fff">1</sup>,
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de" style="color: #fff">Andreas Blattmann</a><sup style="color: #fff">1</sup>,
        <a href="https://github.com/rromb" style="color: #fff">Robin Rombach</a><sup style="color: #fff">1</sup>,
        <a href="https://www.cs.ryerson.ca/kosta/" style="color: #fff">Konstantinos G. Derpanis*</a><sup style="color: #fff">2,3,4</sup>,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer" style="color: #fff">Bj&ouml;rn Ommer*</a><sup style="color: #fff">1</sup><br/>
        <sup style="color: #fff">1</sup><a href="https://www.iwr.uni-heidelberg.de/" style="color: #fff">IWR/HCI, Heidelberg University, Germany</a>
        <sup style="color: #fff">2</sup><a href="https://ryersonvisionlab.github.io/" style="color: #fff">Department of Computer Science, Ryerson University, Canada</a><br/>
        <sup style="color: #fff">3</sup><a href="https://vectorinstitute.ai/" style="color: #fff">Vector Institute for AI, Canada</a>
        <sup style="color: #fff">4</sup><a href="https://research.samsung.com/aicenter_toronto" style="color: #fff">Samsung AI Centre Toronto, Canada</a> <br/>
        <font style="color: #fff"> Accepted to </font> <a href="http://cvpr2021.thecvf.com/" style="color: #fff">CVPR 2021</a><br/>
        </p>
			</section>
        <!-- One -->
        <section id="one" class="wrapper style1">
          <div class="container 75%">
        <b>tl;dr </b> We present a framework for both stochastic and controlled image-to-video synthesis. We bridge the gap between the image and video domain using conditional invertible neural networks and account for the inherent ambiguity with a dedicated, learned scene dynamics representation.
        <br>
        <br> 
            <div class="row 200%">
              <div style="width: 25%">
                              <div class="container 25%">

                    <div class="image fit captioned align-center"
                                style="margin-bottom:0em; box-shadow:0 0">
                      <a href="paper/paper.pdf">
                        <img src="paper/paper.png" alt="" style="border:1px solid black"/>
                      </a>
                      <a href="https://arxiv.org/abs/2105.04551">arXiv</a>
                      <div class="headerDivider"></div>
                      <a href="paper/paper.bib">BibTeX</a>
                      <div class="headerDivider"></div>
                      <a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">GitHub</a>
                      <br/>
                      <a href="results/222_supp.zip">Supplemental</a>
                                  </div>
                              </div>
              </div>
              <div style="text-align: justify; margin-left: 10%; width: 65%">
                            <h1>Abstract</h1>
                  <p>Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results.
                  </p>
              </div>
            </div>
          </div>
        </section>


			<!-- Two -->
              <section id="two" class="wrapper style2 special">
            <div class="container">
                <header class="major">
                    <h2>Approach</h2>
                </header>

                <div class="container 75%">
            <div class="image fit captioned align-left"
                        style="margin-bottom:2em; box-shadow:0 0;
                        text-align:justify">
              <img src="paper/method.png" alt="" style="border:0px solid black"/>
                Overview of our proposed framework. We learn an information preserving video representation \(z\) using our conditional generative model consisting of an encoder \(q_\phi\) as well as the corresponding decoder \(p_\psi\). The decoder consists of dedicated video residual blocks shown in right bottom. 

                After establishing the video representation, we learn a bijective transformation \(\mathcal{T}\) conditioned on the starting frame \(x_0\) and an optionally provided control factor \(\eta\).
                During inference, we sample a residual \(\nu\), encapsulating the scene dynamics, from the prior distribution and use \(\mathcal{T}_\phi\) to obtain the video representation \(z\). Using our decoder we can then synthesize novel video sequences. Training and inference are indicated by the dotted and solid lines respectively.
            </div>
                </div>
            </div>
        </section>


<section id="three" class="wrapper style1 special">
<div class="container">
<header class="major">
  <h2>Results</h2>
</header>


<header class="minor">
  <h2>Results on Landscape</h2>
</header>

<div class="row 150%">
<div class="6u 12u$(xsmall)">


<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Landscape_samples.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Animations from our model given a single conditioning frame. This video corresponds to Fig. 3 in the main paper. 

</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Landscape_diversity.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Visualization of the various animations of the same starting frame. Our model produces diverse outputs capturing different speeds and directions when randomly drawing scene dynamics from the residual space \(\nu \sim q(\nu)\). 
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Landscape_comparison.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
<!-- Comparison with other works using offical pretrained models from the corresponding papers [1, 2, 3].  -->
Qualitative comparison to previous work, i.e., AL[1], DTVNet[2], and MDGAN[3], with `GT' denoting the ground-truth. Both MDGAN[3] and DTVNet[2] produce blurry videos when using the officially provided pretrained weights and code from the respective webpages. While AL produces decent animations in the presence of small motion, when animating fast motions, however, warping artifacts are present, cf. e.g., row 3. In contrast, our method produces realistic looking results in the case of both small and large motions.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/longer_duration.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Visualization of longer sequences (48 frames). Our model was applied sequentially on the last frame of the previously predicted video sequence using the same \(\nu \).
</div>
</div>
</div>

<div class="row 150%">
<div class="image fit captioned align-just">
<div class="videocontainer43ar">
<img src="results/Landscape_more.gif"/>
</div>
More animations from our model which can be generated using our code provided on our 
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
GitHub page.
</a>
</div>
</div>


<header class="minor">
  <h2>Results on BAIR</h2>
</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/BAIR_comparison.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Qualitative comparison with IVRNN [4]. While both approaches are able to render the robot's end effector and the visible environment well, we observe significant differences when it comes to the effector interacting with or occluding background objects. An example of this difficulty can be seen when interacting with the object in the middle of the scene in row 2.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/BAIR_diversity.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
This visualization qualitatively illustrates the prediction diversity of our model by animating a fixed starting frame \(x_0\) multiple times. `GT' denotes ground-truth. Our model synthesizes diverse samples by broadly covering motions in the \(x\), \(y\), and \(z\) directions.
</div>
</div>
</div>


<div class="row 150%">
<div class="image fit captioned align-just">
<div class="videocontainer43ar">
<img src="results/BAIR_more.gif"/>
</div>
More animations from our model which can be generated using our code provided on our 
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
GitHub page.
</a>
</div>
</div>

<header class="minor">
  <h2>Results on Dynamic Textures (DTDB)</h2>
</header>
<p>
Qualitative comparison on various dynamic textures with DG [5] and AL [1]. As described in the main paper, DG [5] is directly optimized on test samples, thus overfitting directly to the test distribution. Consequently, we observe that their generations almost perfectly reproduce the ground-truth motion which is most evident for the clouds texture. However, their method suffers from blurring due to optimization using an L2 pixel loss. Similar to the comparisons on Landscape, AL [1] has problems with learning and generating the motion of dynamic textures exhibiting rapid motion changes, such as fire. This is explained by the susceptibility of optical flow to inaccuracies when capturing very fast motion, as well as dynamic patterns outside the scope of optical flow, e.g., flicker. Our model, on the other hand, produces sharp video sequences with realistic looking motions for all textures. Note, for each method one model is trained for each texture.
</p>

</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Fire.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/vegetation.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
</div>
</div>
</div>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Waterfall.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Clouds.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
</div>
</div>
</div>

<div class="row 150%">
<div class="image fit captioned align-just">
<div class="videocontainer42ar">
<img src="results/fire_more.gif"/>
<img src="results/vegetation_more.gif"/>
<img src="results/waterfall_more.gif"/>
<img src="results/clouds_more.gif"/>

</div>
More animations from our model which can be generated using our code provided on our 
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
GitHub page.
</a>
</div>
</div>



<header class="minor">
  <h2>Results on iPER</h2>
</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
  
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/iPER_samples.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Animations from our model given a single conditioning frame. This video corresponds to Fig. 4 in the main paper. 
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/iPER_comparison.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Qualitative comparison with IVRNN [4]. Our method produces more natural motions, e.g., row 3. `GT' denotes ground-truth.
</div>
</div>
</div>


<div class="row 150%">
<div class="image fit captioned align-just">
<div class="videocontainer43ar">
<img src="results/iPER_more.gif"/>
</div>
More animations from our model which can be generated using our code provided on our 
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
GitHub page.
</a>
</div>
</div>

<header class="minor">
  <h2>Results on Controlled Video Synthesis</h2>
</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Transfer_Landscape.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Motion transfer on landscape. The task is to directly transfer a query motion extracted from a given landscape video \(\tilde{X}\) to a random starting frame \(x_0\). Therefore, we extract the residual representation \(\tilde{\nu}\) of \(\tilde{X}_0\) by first obtaining its video representation \(\tilde{z} = q(z|\tilde{X})\) and corresponding residual \(\tilde{\nu} = \mathcal{T}_\theta^{-1}(\tilde{z};\tilde{x}_0)\) with \(\tilde{x}_0\) being the starting frame of \(\tilde{X}\). We use \(\tilde{\nu}\) to animate the starting frame \(x_0\). Our model accurately transfers the query motion, e.g., as the corresponding direction and speed of the clouds, to the target landscape images (rows 1-3, left-to-right).

</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Direction_Clouds2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Visualization of controlled video-to-video synthesis using cloud video sequences from DTDB. We explicitly adjust the initial factor \(\tilde{\eta}\) of an observed video sequence \(\tilde{X}\). To this end, we first obtain its video representation \(\tilde{z} = q_\phi(z|\tilde{X})\) followed by extracting the corresponding residual information \(\tilde{\nu} = \mathcal{T}_\theta^{-1}(\tilde{z};\tilde{x}_0, \tilde{\eta})\). Subsequently, to generate the video sequence depicting our controlled adjustment of \(\tilde{X}\), we simply choose a new value \(\tilde{\eta}=\tilde{\eta}^*\) and perform the image-to-sequence inference process. In each example (second row), the motion direction of the query video (leftmost) is adjusted by the provided control (top row). To highlight that the residual representations \(\nu\) in these cases actually correspond to the query video, we additionally animate the initial image of the query videos by sampling a new residual representation \(\nu \sim q(\nu)\) and apply the same controls (bottom rows). We observe that, while the directions of the synthesized videos are identical, their speeds are significantly different, as desired. 
<!-- In the case of video-to-video synthesis, the movement speed remains the same, in contrast to the image-to-video case, where the movement speed has changed due to the changed residual representation. -->
</div>
</div>
</div>

<div class="row 150%">
<div class="image fit captioned align-just">
<div class="videocontainer60ar">
<img src="results/transfer.gif"/>
<img src="results/transfer2.gif"/>
</div>
More motion transfer samples from our model which can be generated using our code provided on our 
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
GitHub page.
</a>
 First row depicts the original query sequence and the remaining rows the animations with the transferred dynamics.
</div>

</div>


<div class="row 250%">
<div class="6u 12u$(xsmall)">
	
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Direction_Clouds1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
This video illustrates several image-to-video generations examples for controlling the direction of cloud movements with \(\eta\), similar to Fig. 7 in our main paper. We observe that our model renders crisp future progressions (row 2-5) of a given starting frame \(x_0\), while following our provided movement control (top row).
</div>
</div>
<div class="6u$ 12u$(xsmall)">
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="results/Endpoint_BAIR.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
This video illustrates several image-to-video generations while controlling \(\eta = (x,y,z)\), the 3D end effector position, similar to Fig.~6 in our main paper. It shows that, while in each example the effector approximately stops at the provided end position (end frame of GT), its movements between the starting and end frame, which are inferred by the sampled residual representations \(\nu \sim q(\nu)\), exhibit significantly varying and natural progressions. 
</div>
</div>
</div>


<!-- related works ! -->

<section id="four" class="wrapper style1 special">

<div class="container 75%">
<div class="row 250%">
<div class="12u">
<h2>References </h2>
<div class="12u">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
[1] Yuki Endo, Yoshihiro Kanamori, and Shigeru Kuriyama. Animating landscape: self-supervised learning of decoupled motion and appearance for single-image video synthesis. ACM Transactions on Graphics, pages 175:1–175:19, 2019.
<br/>
[2] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2364–2373, 2018.
<br/>
[3] Jiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, and Yunliang Jiang. Dtvnet: Dynamic time-lapse video generation via single still image. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 300–315, 2020.
<br/>
[4] Lluı́s Castrejón, Nicolas Ballas, and Aaron C. Courville. Improved conditional vrnns for video prediction. In Proceedings of the International Conference on Computer Vision (ICCV), pages 7607–7616, 2019.
<br/>
[5] Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Learning dynamic generator model by alternating back-propagation through time. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 5498–5507, 2019.
</div>
</div>
</div>
</div>
</section>

<section id="four" class="wrapper style1 special">

<div class="container 85%">
<div class="row 250%">
<div class="12u">
<h2> Our Related work on video synthesis</h2>
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
Behavior-Driven Synthesis of Human Dynamics
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
<img src="https://compvis.github.io/behavior-driven-video-synthesis/images/first-page.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.
<!-- Insert here interactive video synthesis work from andy -->
<!-- </div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
Behavior-Driven Synthesis of Human Dynamics
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
<img src="images/teaser_behavior.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.8em">
Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively. -->
</div>
</div>
</div>


<!-- <section id="five" class="wrapper style1"> -->
<div class="container 85%">
<div class="row 250%">
<div class="12u">
<h2> Our Related work on visual synthesis</h2>
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/taming-transformers/">
 Taming Transformers for High-Resolution Image Synthesis 
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/taming-transformers/">
<img src="https://compvis.github.io/taming-transformers/paper/teaser.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers. 
<!-- Insert here interactive video synthesis work from andy -->
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/net2net/">
  Network-to-Network Translation with Conditional Invertible Neural Networks
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/net2net/">
<img src="https://compvis.github.io/net2net/paper/teaser.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own. 
</div>
</div>
</div>
</div>
<!-- <section id="six" class="wrapper style1"> -->

</section>


<section id="seven" class="wrapper style3 special"
  style="background-attachment:scroll;background-position:center bottom;">
	<div class="container">
		<header class="major">
			<h2>Acknowledgement</h2>
      <p>
      	This work was started as part of M.D.’s internship at Ryerson University and was supported by the DAAD scholarship, funded by the NSERC Discovery Grantprogram (K.G.D.), in part by the German Research Foundation (DFG) within project 421703927 (B.O.) and the BW Stiftung (B.O.). K.G.D. contributed to this work in his capacity as an Associate Professor at Ryerson University.
      This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
      </p>
		</header>
	</div>
</section>

<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/skel.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
